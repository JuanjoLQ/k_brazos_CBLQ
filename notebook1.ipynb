{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "view-in-github",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanjoLQ/k_brazos_CBLQ/blob/main/notebook1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cde3161a",
      "metadata": {
        "id": "cde3161a"
      },
      "source": [
        "# **Introducción al Problema de los K-Brazos y Métodos de Aprendizaje por Refuerzo**\n",
        "\n",
        "## **1. Contexto del Problema**\n",
        "\n",
        "El problema de los **K-Brazos** (*Multi-Armed Bandit*, MAB) es un problema fundamental en el **Aprendizaje por Refuerzo** y en la toma de decisiones secuenciales bajo incertidumbre. En este escenario, un agente debe elegir repetidamente entre **K opciones** (brazos), cada una de las cuales proporciona una recompensa **desconocida y estocástica**.\n",
        "\n",
        "El objetivo del agente es aprender, a partir de la experiencia, **qué brazo seleccionar** para maximizar su recompensa acumulada a lo largo del tiempo. Para ello, debe equilibrar dos estrategias opuestas:\n",
        "\n",
        "- **Exploración:** Probar nuevos brazos para descubrir cuál es el mejor.\n",
        "- **Explotación:** Elegir el brazo que ha dado las mejores recompensas hasta el momento.\n",
        "\n",
        "Este dilema entre **exploración y explotación** es clave en numerosas aplicaciones reales, como la **publicidad online**, la **medicina personalizada**, los **sistemas de recomendación** y la **optimización de estrategias de inversión**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Tipos de Brazos Implementados**\n",
        "\n",
        "Para modelar las recompensas en el problema de los K-Brazos, hemos utilizado tres distribuciones estadísticas diferentes:\n",
        "\n",
        "### **2.1. Brazo con Distribución Normal**\n",
        "\n",
        "Cada brazo genera recompensas que siguen una **distribución normal** con media **μ** y desviación estándar **σ**:\n",
        "\n",
        "> **X ∼ N(μ, σ²)**\n",
        "\n",
        "- Se usa cuando las recompensas pueden tomar valores continuos.\n",
        "- Ejemplo: *Tiempo de respuesta de un servidor, precios en bolsa, etc.*\n",
        "\n",
        "### **2.2. Brazo con Distribución Bernoulli**\n",
        "\n",
        "Cada brazo devuelve una recompensa **binaria** (0 o 1) con probabilidad **p**:\n",
        "\n",
        "> **X ∼ Bernoulli(p)**\n",
        "\n",
        "- Se emplea en problemas donde cada acción tiene **éxito o fracaso**.\n",
        "- Ejemplo: *Un usuario hace clic en un anuncio o no.*\n",
        "\n",
        "### **2.3. Brazo con Distribución Binomial**\n",
        "\n",
        "Cada brazo devuelve una recompensa que sigue una **distribución binomial** con **n** intentos y probabilidad de éxito **p**:\n",
        "\n",
        "> **X ∼ Bin(n, p)**\n",
        "\n",
        "- Se usa en contextos donde una acción puede tener múltiples intentos exitosos dentro de un periodo.\n",
        "- Ejemplo: *Cantidad de clics en un anuncio tras n visualizaciones.*\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Algoritmos Utilizados**\n",
        "\n",
        "Para resolver el problema de los K-Brazos, se han implementado diferentes estrategias de selección de brazos:\n",
        "\n",
        "### **3.1. Algoritmo ε-Greedy**\n",
        "\n",
        "Este algoritmo combina exploración y explotación de forma simple:\n",
        "\n",
        "- Con probabilidad **ε**, selecciona un brazo **aleatoriamente** (exploración).\n",
        "- Con probabilidad **1 - ε**, selecciona el brazo con **mayor recompensa esperada** (explotación).\n",
        "\n",
        "**Parámetro clave:**\n",
        "- **ε (epsilon):** Controla el grado de exploración (valores típicos: 0.1, 0.01, 0).\n",
        "\n",
        "**Ventajas:**\n",
        "- Simple de implementar.  \n",
        "- Funciona bien en la práctica con un ajuste adecuado de ε.  \n",
        "\n",
        "**Desventajas:**\n",
        "- No ajusta dinámicamente la exploración.\n",
        "\n",
        "---\n",
        "\n",
        "### **3.2. Algoritmo UCB (Upper Confidence Bound)**\n",
        "\n",
        "Este algoritmo balancea exploración y explotación utilizando la **incertidumbre** sobre cada brazo:\n",
        "\n",
        "- Se calcula un **límite superior de confianza** para cada brazo.\n",
        "- Se selecciona el brazo con el **mayor valor**, favoreciendo la exploración al inicio y la explotación posteriormente.\n",
        "\n",
        "**Variantes implementadas:**\n",
        "\n",
        "1. **UCB1:** Calcula los límites superiores como:\n",
        "\n",
        "   > **UCBᵢ = μ̂ᵢ + c · √( log(t) / Nᵢ )**\n",
        "\n",
        "   donde **c** controla el grado de exploración.\n",
        "\n",
        "2. **UCB2:** Introduce un ajuste dinámico basado en la duración de las fases de exploración.\n",
        "\n",
        "**Ventajas:**\n",
        "- Reduce la exploración innecesaria con el tiempo.  \n",
        "- Mejores garantías teóricas que ε-Greedy.\n",
        "\n",
        "**Desventajas:**\n",
        "- Más costoso computacionalmente.\n",
        "\n",
        "---\n",
        "\n",
        "### **3.3. Algoritmos Basados en Gradiente**\n",
        "\n",
        "Estos algoritmos actualizan las **preferencias** de cada brazo utilizando la recompensa obtenida:\n",
        "\n",
        "#### **Softmax**\n",
        "\n",
        "Asigna probabilidades a cada brazo usando la función **softmax**:\n",
        "\n",
        "> **P(aᵢ) = exp(Hᵢ) / Σ exp(Hⱼ)**\n",
        "\n",
        "Donde **Hᵢ** es la preferencia del brazo i, actualizada tras cada iteración.\n",
        "\n",
        "#### **Gradient Bandit**\n",
        "\n",
        "Variante que ajusta las preferencias usando una **tasa de aprendizaje**, teniendo en cuenta la media de las recompensas.\n",
        "\n",
        "**Ventajas:**\n",
        "- Adecuado para recompensas con estructura continua.  \n",
        "- Ajusta dinámicamente exploración/explotación.\n",
        "\n",
        "**Desventajas:**\n",
        "- Sensible a la tasa de aprendizaje.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Métricas de Evaluación**\n",
        "\n",
        "Para comparar el rendimiento de los algoritmos, se utilizan las siguientes métricas:\n",
        "\n",
        "1. **Recompensa Promedio**\n",
        "   - Evalúa cómo evoluciona la recompensa media con el tiempo.\n",
        "   - Indica qué estrategia aprende más rápido y obtiene mejores resultados.\n",
        "\n",
        "2. **Porcentaje de Selección del Brazo Óptimo**\n",
        "   - Mide con qué frecuencia se selecciona el mejor brazo.\n",
        "   - Un valor alto implica una buena capacidad de aprendizaje.\n",
        "\n",
        "3. **Regret Acumulado**\n",
        "   - Cuantifica la pérdida por no haber elegido siempre la mejor acción.\n",
        "   - Cuanto menor sea el **regret**, mejor es el algoritmo.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Conclusión**\n",
        "\n",
        "Este estudio analiza distintos enfoques para resolver el problema de los **K-Brazos**, comparando estrategias como **ε-Greedy**, **UCB** y **métodos basados en gradiente**. Además, se han evaluado distintas distribuciones de recompensa para observar su impacto en el comportamiento del agente.\n",
        "\n",
        "**Resumen de resultados esperados:**\n",
        "\n",
        "- **ε-Greedy (ε = 0.1)** suele ser una opción robusta en muchos escenarios.\n",
        "- **UCB** permite una exploración más eficiente, especialmente en etapas tempranas.\n",
        "- **Métodos de gradiente** se adaptan bien cuando las recompensas presentan estructuras complejas.\n",
        "- La **distribución de los brazos** afecta significativamente la velocidad de aprendizaje y el rendimiento.\n",
        "\n",
        "---\n",
        "\n",
        "**Nota:** Todos los experimentos han sido desarrollados en **Python**, utilizando librerías como `numpy` y `matplotlib`, y están organizados en módulos para facilitar su ejecución y análisis.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
